{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "793cb98e",
   "metadata": {},
   "source": [
    "# Complete Loss Functions Summary\n",
    "\n",
    "## Total Loss Function:\n",
    "\n",
    "$$L_{total} = \\lambda_1 L_{GAEX} + \\lambda_2 L_{GAEA} + \\lambda_3 L_{clu} + \\lambda_4 L_{gae} + \\lambda_5 L_{ZINB}$$\n",
    "\n",
    "**Hyperparameter Values:**\n",
    "- $\\lambda_1 = 0.5$ (Graph expression weight)\n",
    "- $\\lambda_2 = 0.01$ (Graph adjacency weight)  \n",
    "- $\\lambda_3 = 0.1$ (ZINB clustering weight)\n",
    "- $\\lambda_4 = 0.01$ (Graph clustering weight)\n",
    "- $\\lambda_5 = 0.5$ (ZINB reconstruction weight)\n",
    "\n",
    "---\n",
    "\n",
    "## Loss 1: ZINB Reconstruction Loss\n",
    "\n",
    "$$L_{ZINB} = -\\log(\\text{ZINB}(\\bar{X}|\\pi, \\mu, \\theta))$$\n",
    "\n",
    "**ZINB Distribution:**\n",
    "$$\\text{ZINB}(\\bar{X}|\\pi, \\mu, \\theta) = \\pi \\cdot \\delta_0(\\bar{X}) + (1-\\pi) \\cdot \\text{NB}(\\bar{X}|\\mu, \\theta)$$\n",
    "\n",
    "**Negative Binomial:**\n",
    "$$\\text{NB}(\\bar{X}|\\mu, \\theta) = \\frac{\\Gamma(\\bar{X}+\\theta)}{\\bar{X}!\\Gamma(\\theta)}\\left(\\frac{\\theta}{\\theta+\\mu}\\right)^\\theta\\left(\\frac{\\mu}{\\theta+\\mu}\\right)^{\\bar{X}}$$\n",
    "\n",
    "**Purpose:** Model zero-inflation and overdispersion in scRNA-seq data\n",
    "\n",
    "---\n",
    "\n",
    "## Loss 2: Graph Adjacency Reconstruction Loss\n",
    "\n",
    "$$L_{GAEA} = \\|A - \\hat{A}\\|_F^2$$\n",
    "\n",
    "Where: $\\hat{A} = \\text{sigmoid}(Z_L^T Z_L)$\n",
    "\n",
    "**Purpose:** Preserve structural relationships between cells\n",
    "\n",
    "---\n",
    "\n",
    "## Loss 3: Graph Expression Preservation Loss\n",
    "\n",
    "$$L_{GAEX} = \\|\\bar{X} - Z_L\\|_F^2$$\n",
    "\n",
    "**Purpose:** Ensure GCN output retains gene expression information\n",
    "\n",
    "---\n",
    "\n",
    "## Loss 4: ZINB Module Clustering Loss\n",
    "\n",
    "$$L_{clu} = KL(P \\| Q) = \\sum_{i}\\sum_{j} p_{ij} \\log\\frac{p_{ij}}{q_{ij}}$$\n",
    "\n",
    "**Applied to:** $H_{L/2}$ (middle layer of ZINB autoencoder)\n",
    "\n",
    "**Purpose:** Guide clustering from content perspective\n",
    "\n",
    "---\n",
    "\n",
    "## Loss 5: GCN Module Clustering Loss  \n",
    "\n",
    "$$L_{gae} = KL(P \\| Z_{pre}) = \\sum_{i}\\sum_{j} p_{ij} \\log\\frac{p_{ij}}{z_{ij}}$$\n",
    "\n",
    "Where: $Z_{pre} = \\text{softmax}\\left(\\hat{D}^{-\\frac{1}{2}}(A+I)\\hat{D}^{-\\frac{1}{2}}R_{L/2-1}U_{L/2-1}\\right)$\n",
    "\n",
    "**Applied to:** Graph autoencoder predictions\n",
    "\n",
    "**Purpose:** Guide clustering from structural perspective\n",
    "\n",
    "---\n",
    "\n",
    "## Soft Assignment & Target Distribution:\n",
    "\n",
    "### Soft Assignment (Q):\n",
    "$$q_{ij} = \\frac{\\left[1 + \\frac{\\|h_i - \\mu_j\\|^2}{\\lambda}\\right]^{-\\frac{(\\lambda+1)}{2}}}{\\sum_{j'} \\left[1 + \\frac{\\|h_i - \\mu_{j'}\\|^2}{\\lambda}\\right]^{-\\frac{(\\lambda+1)}{2}}}$$\n",
    "\n",
    "### Target Distribution (P):\n",
    "$$p_{ij} = \\frac{q_{ij}^2 / g_j}{\\sum_{j'} q_{ij'}^2 / g_{j'}}$$\n",
    "\n",
    "Where: $g_j = \\sum_i q_{ij}$ (soft cluster frequency)\n",
    "\n",
    "---\n",
    "\n",
    "## Training Strategy:\n",
    "\n",
    "### Phase 1: Pre-training (100 epochs)\n",
    "- **Loss:** $L_{ZINB}$ only\n",
    "- **Purpose:** Initialize ZINB autoencoder\n",
    "\n",
    "### Phase 2: Joint Training (200 epochs)  \n",
    "- **Loss:** $L_{total}$ (all 5 components)\n",
    "- **Purpose:** Optimize all modules simultaneously\n",
    "- **Convergence:** Stop when cluster assignment changes < 0.1%\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights:\n",
    "\n",
    "1. **Five complementary objectives** working together\n",
    "2. **Dual clustering losses** from both ZINB and GCN modules  \n",
    "3. **Balanced weights** emphasize reconstruction (0.5) and structure (0.01-0.1)\n",
    "4. **End-to-end training** with self-supervision\n",
    "5. **ZINB modeling** handles scRNA-seq characteristics\n",
    "6. **Graph regularization** preserves cell relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc74f649",
   "metadata": {},
   "source": [
    "# Module 4: Self-Supervised Learning\n",
    "\n",
    "## Objective:\n",
    "Enable end-to-end clustering through soft assignments and target distribution learning\n",
    "\n",
    "---\n",
    "\n",
    "## Soft Assignment (Student's t-distribution):\n",
    "\n",
    "$$q_{ij} = \\frac{\\left[1 + \\frac{\\|h_i - \\mu_j\\|^2}{\\lambda}\\right]^{-\\frac{(\\lambda+1)}{2}}}{\\sum_{j'} \\left[1 + \\frac{\\|h_i - \\mu_{j'}\\|^2}{\\lambda}\\right]^{-\\frac{(\\lambda+1)}{2}}}$$\n",
    "\n",
    "Where:\n",
    "- $q_{ij}$ = soft assignment probability of sample $i$ to cluster $j$\n",
    "- $h_i$ = embedding representation of sample $i$\n",
    "- $\\mu_j$ = cluster center $j$\n",
    "- $\\lambda$ = degrees of freedom for Student's t-distribution (typically λ = 1)\n",
    "- $\\|h_i - \\mu_j\\|^2$ = squared Euclidean distance\n",
    "\n",
    "**Purpose**: Compute soft cluster assignments using Student's t-distribution\n",
    "\n",
    "---\n",
    "\n",
    "## Target Distribution (High-Confidence):\n",
    "\n",
    "$$p_{ij} = \\frac{q_{ij}^2 / g_j}{\\sum_{j'} q_{ij'}^2 / g_{j'}}$$\n",
    "\n",
    "Where:\n",
    "- $p_{ij}$ = target distribution (sharpened assignments)\n",
    "- $q_{ij}$ = soft assignment from above\n",
    "- $g_j = \\sum_i q_{ij}$ = soft cluster frequency\n",
    "\n",
    "**Purpose**: Create high-confidence target distribution to guide learning\n",
    "\n",
    "---\n",
    "\n",
    "## How Self-Supervised Learning Works:\n",
    "\n",
    "### Step 1: Initialize Cluster Centers\n",
    "- Apply k-means clustering on $H_{L/2}$ (middle layer of ZINB autoencoder)\n",
    "- Obtain initial cluster centers: $\\{\\mu_1, \\mu_2, ..., \\mu_k\\}$\n",
    "\n",
    "### Step 2: Compute Soft Assignments\n",
    "- Calculate $q_{ij}$ using Student's t-distribution\n",
    "- Each cell gets probability distribution over clusters\n",
    "- Soft assignments allow gradual refinement\n",
    "\n",
    "### Step 3: Generate Target Distribution  \n",
    "- Compute cluster frequencies: $g_j = \\sum_i q_{ij}$\n",
    "- Create sharpened distribution: $p_{ij} = \\frac{q_{ij}^2/g_j}{\\sum_{j'} q_{ij'}^2/g_{j'}}$\n",
    "- Target distribution emphasizes high-confidence assignments\n",
    "\n",
    "### Step 4: Update Parameters\n",
    "- Use KL divergence loss to align Q with P\n",
    "- Update both network parameters and cluster centers\n",
    "- Iterative refinement improves clustering\n",
    "\n",
    "---\n",
    "\n",
    "## Key Properties:\n",
    "\n",
    "### Student's t-Distribution Benefits:\n",
    "1. **Heavy tails**: Robust to outliers\n",
    "2. **Smooth gradients**: Better optimization than hard assignments\n",
    "3. **Adaptive**: Adjusts based on distance to cluster centers\n",
    "4. **Normalized**: Probabilities sum to 1 for each sample\n",
    "\n",
    "### Target Distribution Benefits:\n",
    "1. **Sharpening**: Emphasizes confident predictions\n",
    "2. **Frequency normalization**: Prevents trivial solutions\n",
    "3. **High confidence**: Guides model toward decisive clustering\n",
    "4. **Balanced**: Accounts for cluster size differences\n",
    "\n",
    "---\n",
    "\n",
    "## Dual Self-Supervision:\n",
    "- Applied to **both** ZINB module ($H_{L/2}$) and GCN module ($Z_{pre}$)\n",
    "- Same target distribution $P$ guides both modules\n",
    "- Ensures unified clustering objective across architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e4288",
   "metadata": {},
   "source": [
    "# Module 3: Attention Fusion Mechanism\n",
    "\n",
    "## Objective:\n",
    "Intelligently integrate gene expression + structural information\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Head Attention (8 heads):\n",
    "\n",
    "### Step 1: Weighted Combination\n",
    "\n",
    "$$Y_{l-1} = \\alpha \\times H_{l-1} + (1-\\alpha) \\times Z_{l-1}$$\n",
    "\n",
    "Where:\n",
    "- $Y_{l-1}$ = combined representation\n",
    "- $\\alpha = 0.5$ (balance parameter)\n",
    "- $H_{l-1}$ = output from ZINB autoencoder\n",
    "- $Z_{l-1}$ = output from graph autoencoder\n",
    "- $(1-\\alpha) = 0.5$ = complementary weight\n",
    "\n",
    "**Purpose**: Balance content and structural information equally\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Multi-Head Attention\n",
    "\n",
    "$$\\text{head}_i = \\text{softmax}\\left(\\frac{Q \\times K^T}{\\sqrt{d_k}}\\right) \\times V$$\n",
    "\n",
    "$$R_l = W \\times \\text{Concat}(\\text{head}_1, ..., \\text{head}_8)$$\n",
    "\n",
    "Where:\n",
    "- $\\text{head}_i$ = output of attention head $i$\n",
    "- $Q$ = query matrix ($Q = W_i^Q \\times Y_{l-1}$)\n",
    "- $K$ = key matrix ($K = W_i^K \\times Y_{l-1}$)\n",
    "- $V$ = value matrix ($V = W_i^V \\times Y_{l-1}$)\n",
    "- $d_k$ = dimension of key vectors\n",
    "- $\\sqrt{d_k}$ = scaling factor (prevents large dot products)\n",
    "- $W$ = weight matrix for final projection\n",
    "- $\\text{Concat}$ = concatenation operation\n",
    "- Number of heads = 8\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works:\n",
    "\n",
    "### For each attention head $i$ (i = 1, 2, ..., 8):\n",
    "\n",
    "1. **Transform input** into Q, K, V using learned weight matrices\n",
    "   - $Q_i = W_i^Q \\times Y_{l-1}$\n",
    "   - $K_i = W_i^K \\times Y_{l-1}$\n",
    "   - $V_i = W_i^V \\times Y_{l-1}$\n",
    "\n",
    "2. **Compute attention scores**\n",
    "   - Calculate similarity: $Q_i \\times K_i^T$\n",
    "   - Scale by $\\sqrt{d_k}$ to prevent gradient issues\n",
    "   - Apply softmax to get attention weights\n",
    "\n",
    "3. **Apply attention** to values\n",
    "   - Weighted sum: $\\text{attention\\_weights} \\times V_i$\n",
    "\n",
    "4. **Concatenate all heads** and project\n",
    "   - Combine: $[\\text{head}_1 | \\text{head}_2 | ... | \\text{head}_8]$\n",
    "   - Final transformation: $R_l = W \\times \\text{concatenated\\_heads}$\n",
    "\n",
    "---\n",
    "\n",
    "## Key Benefits:\n",
    "\n",
    "### 1. Adaptive Feature Weighting\n",
    "- Different features get different importance automatically\n",
    "- Model learns which genes/structures matter most\n",
    "\n",
    "### 2. Multi-Perspective Learning\n",
    "- 8 heads capture diverse patterns simultaneously\n",
    "- Each head focuses on different aspects\n",
    "\n",
    "### 3. Layer-by-Layer Fusion\n",
    "- Fusion happens at each layer, not just once\n",
    "- Prevents information loss through depth\n",
    "\n",
    "### 4. Balanced Integration\n",
    "- α = 0.5 ensures equal contribution from both modules\n",
    "- No dominance of content or structure\n",
    "\n",
    "### 5. Prevents Oversmoothing\n",
    "- Maintains discriminative features from ZINB module\n",
    "- Combats GCN's tendency to blur information\n",
    "\n",
    "---\n",
    "\n",
    "## Attention Mechanism Advantages:\n",
    "\n",
    "- **Selective**: Focuses on relevant information\n",
    "- **Dynamic**: Adapts based on input data\n",
    "- **Interpretable**: Attention weights show importance\n",
    "- **Powerful**: Captures complex dependencies\n",
    "- **Scalable**: Parallel computation across heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40425105",
   "metadata": {},
   "source": [
    "# Module 2: Graph Autoencoder (GCN)\n",
    "\n",
    "## Objective:\n",
    "Capture high-order structural relationships between cells\n",
    "\n",
    "---\n",
    "\n",
    "## Graph Convolutional Network Layer:\n",
    "\n",
    "$$Z_l = \\text{ReLU}\\left(\\hat{D}^{-\\frac{1}{2}} \\times (A+I) \\times \\hat{D}^{-\\frac{1}{2}} \\times R_{l-1} \\times U_{l-1}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $Z_l$ = output of GCN layer $l$\n",
    "- $\\hat{D}$ = degree matrix\n",
    "- $A$ = adjacency matrix (KNN graph)\n",
    "- $I$ = identity matrix\n",
    "- $R_{l-1}$ = fused representation from previous layer (attention output)\n",
    "- $U_{l-1}$ = weight parameters of layer $l-1$\n",
    "- $\\text{ReLU}$ = activation function\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features:\n",
    "\n",
    "### A = KNN Adjacency Matrix (k=10)\n",
    "- Represents cell-cell relationships\n",
    "- K-nearest neighbor graph construction\n",
    "- Each cell connected to its 10 most similar neighbors\n",
    "- Captures local structure in expression space\n",
    "\n",
    "### I = Identity Matrix\n",
    "- Self-connections for each node\n",
    "- Ensures node's own features are included\n",
    "- Prevents information loss in aggregation\n",
    "\n",
    "### D̂ = Degree Matrix\n",
    "- Diagonal matrix with node degrees\n",
    "- $\\hat{D}_{ii} = \\sum_j (A_{ij} + I_{ij})$\n",
    "- Used for symmetric normalization\n",
    "- Balances influence of neighbors\n",
    "\n",
    "### R_(l-1) = Fused Representation (Attention Output)\n",
    "- Combined content + structure information\n",
    "- Output from attention fusion module\n",
    "- Prevents oversmoothing\n",
    "- Maintains discriminative features\n",
    "\n",
    "### Mitigates Oversmoothing Problem in Deep GCNs\n",
    "- **Problem**: Deep GCNs blur node features\n",
    "- **Solution**: Inject content information via $R_{l-1}$\n",
    "- Preserves cell-specific characteristics\n",
    "- Enables deeper network architectures\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits:\n",
    "\n",
    "1. **Structural Learning**: Captures cell-cell relationships\n",
    "2. **High-order Information**: Multi-layer aggregation\n",
    "3. **Normalized Aggregation**: Symmetric normalization prevents scaling issues\n",
    "4. **Feature Preservation**: Fused input maintains content information\n",
    "5. **Robust to Noise**: Graph structure filters technical noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d9412",
   "metadata": {},
   "source": [
    "# Module 1: ZINB-based Autoencoder\n",
    "\n",
    "## Objective:\n",
    "Learn low-dimensional representations capturing gene expression patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Formulation:\n",
    "\n",
    "### Encoder:\n",
    "$$H_l = \\text{ReLU}(W_l \\times H_{l-1} + b_l)$$\n",
    "\n",
    "Where:\n",
    "- $H_l$ = output of layer $l$\n",
    "- $W_l$ = weight matrix of layer $l$\n",
    "- $b_l$ = bias vector of layer $l$\n",
    "- $\\text{ReLU}$ = Rectified Linear Unit activation function\n",
    "\n",
    "---\n",
    "\n",
    "## ZINB Parameters:\n",
    "\n",
    "### Dropout Parameter:\n",
    "$$\\Pi = \\text{sigmoid}(W_\\pi \\times H_L)$$\n",
    "\n",
    "### Mean Parameter:\n",
    "$$M_i = \\text{diag}(S_i) \\times \\exp(W_\\mu \\times H_L)$$\n",
    "\n",
    "### Dispersion Parameter:\n",
    "$$\\Theta = \\exp(W_\\theta \\times H_L)$$\n",
    "\n",
    "Where:\n",
    "- $\\Pi$ (pi) = dropout probability (zero-inflation)\n",
    "- $M_i$ = mean parameter for cell $i$\n",
    "- $S_i$ = size factor for cell $i$\n",
    "- $\\Theta$ (theta) = dispersion parameter (overdispersion)\n",
    "- $H_L$ = final layer output\n",
    "\n",
    "---\n",
    "\n",
    "## Network Architecture:\n",
    "\n",
    "### Encoder Layers:\n",
    "```\n",
    "Input: 2000 genes\n",
    "  ↓\n",
    "Dense Layer 1: 1000 nodes + ReLU\n",
    "  ↓\n",
    "Dense Layer 2: 1000 nodes + ReLU\n",
    "  ↓\n",
    "Dense Layer 3: 4000 nodes + ReLU\n",
    "  ↓\n",
    "Latent Layer: 10 nodes + ReLU\n",
    "```\n",
    "\n",
    "**Architecture Flow:**\n",
    "$$2000 \\rightarrow 1000 \\rightarrow 1000 \\rightarrow 4000 \\rightarrow 10 \\text{ nodes}$$\n",
    "\n",
    "### Decoder Layers (Symmetric):\n",
    "```\n",
    "Latent: 10 nodes\n",
    "  ↓\n",
    "Dense Layer 1: 4000 nodes + ReLU\n",
    "  ↓\n",
    "Dense Layer 2: 1000 nodes + ReLU\n",
    "  ↓\n",
    "Dense Layer 3: 1000 nodes + ReLU\n",
    "  ↓\n",
    "Output: 2000 genes (reconstructed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a7f6f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Features:\n",
    "\n",
    "1. **Dimensionality Reduction**: 2000 → 10 dimensions\n",
    "2. **Zero-Inflation Modeling**: Handles dropout events via $\\Pi$ parameter\n",
    "3. **Overdispersion Handling**: Captures variance via $\\Theta$ parameter\n",
    "4. **Symmetric Architecture**: Encoder-decoder structure\n",
    "5. **ReLU Activation**: Non-linear transformations throughout\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose:\n",
    "\n",
    "- Capture **content information** from gene expression data\n",
    "- Model the **statistical distribution** of scRNA-seq data\n",
    "- Learn **meaningful latent representations** for clustering\n",
    "- Handle **sparsity and noise** inherent in single-cell data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
